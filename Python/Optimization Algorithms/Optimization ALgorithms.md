
## Limitation of Gradient Descent Algorithm :
The initial initialization of weights in Gradient descent algorithm happens randomly. In flat regions of the loss functions the derivative of loss w.r.t weights has very low value , thus the update in weights and biases is very low. Thus to escape from the flat regions will take time for the gradient descent algorithm. While in contrast the derivative of loss w.r.t weights in steep regions is very high thus the update to weights in back propogation is high. thus faster movement.

